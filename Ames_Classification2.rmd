---
output:
  word_document: default
  html_document: default
---
# Ames Classification stack

### Brent Slater

```{r, include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) 
library(stacks)
```

```{r}
ames_student <- read_csv("ames_student.csv")
```

```{r}
ames_student = ames_student %>% mutate_if(is.character, as_factor)
```

```{r}
set.seed(123) 
ames_split = initial_split(ames_student, prop = 0.7, strata = Above_Median) #70% in training
train = training(ames_split)
test = testing(ames_split)
```

```{r}
set.seed(123)
folds = vfold_cv(train, v = 5)
```

```{r}
ames_recipe = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + Garage_Cars + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Year_Remod_Add + Foundation, train)

ctrl_grid = control_stack_grid()
ctrl_res = control_stack_resamples() 
```

Class tree Model
```{r}
 # tree_model = decision_tree(cost_complexity = tune()) %>% 
# set_engine("rpart", model = TRUE) %>% 
 # set_mode("classification")

#tree_grid = expand.grid(cost_complexity = seq(0.001,0.02,by=0.001))

#tree_recipe = ames_recipe %>%
 # step_dummy(all_nominal(),-all_outcomes())

#tree_workflow = workflow() %>%
#  add_model(tree_model) %>%
 # add_recipe(tree_recipe)

#set.seed(1234)
#tree_res = 
#  tree_workflow %>% 
#  tune_grid(
 #   resamples = folds,
#    grid = 25, #try 25 reasonable values for cp
 #   control = ctrl_grid #needed for stacking
#    )
```
```{r}
#saveRDS(tree_res,"tree_res.rds")
```

```{r}
tree_res = readRDS("tree_res.rds")
```

```{r}
tree_res %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 
```

```{r}
best_tree = tree_res %>% 
  select_best("accuracy")

best_tree
```

```{r}
#final_wf = 
  #tree_workflow %>% 
 # finalize_workflow(best_tree)
  
```

```{r}
#final_fittree = fit(final_wf, train)

#tree = final_fittree %>% 
  #pull_workflow_fit() %>% 
 # pluck("fit")
```


Random Forest Model 
```{r}
#rf_recipe = tree_recipe %>%
 #  step_dummy(all_nominal(), -all_outcomes())
 #
# rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% #add tuning of mtry and min_n #parameters
 #  set_engine("ranger", importance = "permutation") %>% #added importance metric
  # set_mode("classification")
 #rf_wflow = 
  #workflow() %>% 
   #add_model(rf_model) %>% 
#  add_recipe(rf_recipe)
 
 #rf_grid = grid_regular(
  #mtry(range = c(2, 4)), #these values determined through significant trial and error
  #min_n(range = c(10, 40)), #these values determined through significant trial and error
  #levels = 5
#)
 
 #set.seed(1234)
#rf_res = tune_grid(
 #  rf_wflow,
  # resamples = folds,
  # grid = 100, 
   #control = ctrl_grid
#)
```

```{r}
#saveRDS(rf_res,"rf_res.rds")
```

```{r}
rf_res = readRDS("rf_res.rds")
```


```{r}
rf_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "Accuracy")
```
neural network 
```{r}

#neural_grid = grid_regular(
 # hidden_units(range = c(1,8)),
  #penalty(range = c(-10,-1)),
  #epochs(range = c(1,100)),
  #levels = 10
#) 

#nn_recipe = ames_recipe %>%
 #  step_normalize(all_predictors(), -all_nominal()) #normalize the numeric predictors, not needed for #categorical
 
 #nn_model =
  # mlp(hidden_units = tune(), penalty = tune(),
   #    epochs = tune()) %>%
   #set_mode("classification") %>%
   #set_engine("nnet", verbose = 0) #verbose = 0 reduces output from the model
 
 #nn_workflow <-
  # workflow() %>%
#add_recipe(nn_recipe) %>%
 #  add_model(nn_model)
 
 #set.seed(1234)
 #neural_res <-
  # tune_grid(nn_workflow,
   #          resamples = folds,
    #         grid = 100,
     #        control = ctrl_grid)
```

```{r}
#saveRDS(neural_res,"neural_res.rds")
```

```{r}
neural_res = readRDS("neural_res.rds")
```

```{r}
neural_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, hidden_units, penalty, epochs) %>%
  pivot_longer(hidden_units:epochs,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```


xgb boost model 
```{r}
#start_time = Sys.time() #for timing

#tgrid = expand.grid(
 # trees = 100, #50, 100, and 150 in default 
  #min_n = 1, #fixed at 1 as default 
  #tree_depth = c(1,2,3,4), #1, 2, and 3 in default 
  #learn_rate = c(0.01, 0.1, 0.2, 0.3, 0.4), #0.3 and 0.4 in default 
  #loss_reduction = 0, #fixed at 0 in default 
  #sample_size = c(0.5, 0.8, 1)) #0.5, 0.75, and 1 in default, 

#xgboost_recipe <- 
 # recipe(formula = Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + Garage_Cars + First_Flr_SF + #Total_Bsmt_SF + Garage_Area + Year_Remod_Add + Foundation, data = train) %>% 
  #step_novel(all_nominal(), -all_outcomes()) %>% 
 # step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
  #step_zv(all_predictors()) 

#xgboost_spec <- 
 # boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
  #  loss_reduction = tune(), sample_size = tune()) %>% 
  #set_mode("classification") %>% 
  #set_engine("xgboost") 

#xgboost_workflow <- 
 # workflow() %>% 
 # add_recipe(xgboost_recipe) %>% 
#  add_model(xgboost_spec) 

#set.seed(1234)
#xgb_res <-
 # tune_grid(xgboost_workflow, 
#            resamples = folds, 
 #           grid = tgrid,
  #          control = ctrl_grid)

#end_time = Sys.time()
#end_time-start_time
```

```{r}
#saveRDS(xgb_res,"xgb_res.rds")
```

```{r}
xgb_res = readRDS("xgb_res.rds")
```

```{r}
ames_stacks = stacks() %>%
  add_candidates(tree_res) %>%
  add_candidates(rf_res) %>% 
   add_candidates(neural_res) %>%
  add_candidates(xgb_res)
```

```{r}
ames_blend = 
  ames_stacks %>% 
  blend_predictions(metric = metric_set(accuracy))
```

```{r}
autoplot(ames_blend, type = "weights")
```

```{r}
ames_blend <-
  ames_blend %>%
  fit_members()
```


```{r}
trainpredstack = predict(ames_blend, train)
head(trainpredstack)
```

```{r}
confusionMatrix(trainpredstack$.pred_class, train$Above_Median, 
                positive = "Yes")
```

```{r}
testpredstack = predict(ames_blend, test)
head(testpredstack)
```

```{r}
confusionMatrix(testpredstack$.pred_class, test$Above_Median, 
                positive = "Yes")
```
There was a 8% degredation from the training to testing set which suggests there may be some overfitting in the model 
```{r}
test = test %>% bind_cols(predict(ames_blend,.))
```

```{r}
member_testpreds =  
  test %>%
  select(Above_Median) %>%
  bind_cols(predict(ames_blend, test, members = TRUE))
```

```{r}
map_dfr(member_testpreds, accuracy, truth = Above_Median, data = member_testpreds) %>%
  mutate(member = colnames(member_testpreds))
```
making predictions on the competition data 
```{r}
#comppred = predict(ames_blend, competition)
#head(comppred)
```
```
There was only a 5% degradation of the model on the competition test data. I could improve the model stack with more tuning of the individual models. I also think a stack of just xgb and rf models could be best since they seem to be perfroming the best. 





